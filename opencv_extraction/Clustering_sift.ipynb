{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will classify the pictures using a Bag of Words approach. <br> <br>\n",
    "The sifts of each train and test pictures has been extracted after different types preprocessing on the pictures:\n",
    "- Resizing, Transforming to gray scale and extracting every sift\n",
    "- Denoizing, Resizing, Transforming to gray scale and extracting every sift\n",
    "- Resizing, Transforming to gray scale, extracting the sifts the dense way (dividing the pictures into square, and exctracting one sift per square) <br>\n",
    "\n",
    "Here we will cluster the previously extracted sifts to obtain a dictionnary, count the number of \"words\" per pictures using different weighting and build our models on top of that <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import and process the data: <br> \n",
    "We have two datasets of sifts per preprocessing techniques we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    0    1    2    3    4    5    6    7    8   ...      119  \\\n",
       "0           0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   ...     14.0   \n",
       "1           1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   ...     33.0   \n",
       "2           2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   ...      0.0   \n",
       "3           3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   ...      0.0   \n",
       "4           4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   ...      4.0   \n",
       "\n",
       "     120   121  122  123  124   125    126   127       Id  \n",
       "0    0.0   0.0  0.0  0.0  0.0  20.0  255.0   8.0 -1191173  \n",
       "1   99.0   4.0  0.0  0.0  0.0   0.0    2.0  40.0 -1191173  \n",
       "2  175.0  58.0  0.0  0.0  0.0   0.0    0.0   0.0 -1191173  \n",
       "3  177.0  62.0  0.0  0.0  0.0   0.0    0.0   0.0 -1191173  \n",
       "4  176.0   7.0  0.0  0.0  0.0   0.0    0.0   4.0 -1191173  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the sift descriptors files (dense)\n",
    "sifts_train = pd.read_csv('train_sifts_dense.csv')\n",
    "sifts_test = pd.read_csv('test_sifts_dense.csv')\n",
    "sifts_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sifts_train dataframe contains all the sifts descriptors extracted for each train picture:\n",
    "- Unnamed: 0 : Id of the sift in the picture \n",
    "- 0 - 127 : Sift descriptor values\n",
    "- Id : Id of the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train:', 0, 'pictures without any sifts descriptors')\n",
      "('Test:', 0, 'pictures without any sifts descriptors')\n"
     ]
    }
   ],
   "source": [
    "print('Train:', 8000 - len(np.unique(sifts_train['Id'])), 'pictures without any sifts descriptors')\n",
    "print('Test:', 13999 - len(np.unique(sifts_test['Id'])), 'pictures without any sifts descriptors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>4.664526</td>\n",
       "      <td>-0.130893</td>\n",
       "      <td>-0.517853</td>\n",
       "      <td>-0.479069</td>\n",
       "      <td>-0.491777</td>\n",
       "      <td>-0.475926</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>0.124039</td>\n",
       "      <td>5.052102</td>\n",
       "      <td>-0.244152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>0.376582</td>\n",
       "      <td>1.650301</td>\n",
       "      <td>-0.360755</td>\n",
       "      <td>-0.491777</td>\n",
       "      <td>-0.475926</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.463781</td>\n",
       "      <td>0.676013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>-0.504821</td>\n",
       "      <td>3.314742</td>\n",
       "      <td>1.236482</td>\n",
       "      <td>-0.491777</td>\n",
       "      <td>-0.475926</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.507385</td>\n",
       "      <td>-0.474193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>-0.504821</td>\n",
       "      <td>3.358543</td>\n",
       "      <td>1.354796</td>\n",
       "      <td>-0.491777</td>\n",
       "      <td>-0.475926</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.507385</td>\n",
       "      <td>-0.474193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>-0.397985</td>\n",
       "      <td>3.336642</td>\n",
       "      <td>-0.272019</td>\n",
       "      <td>-0.491777</td>\n",
       "      <td>-0.475926</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.507385</td>\n",
       "      <td>-0.359172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6    \\\n",
       "0 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "1 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "2 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "3 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "4 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "\n",
       "        7         8         9      ...          118       119       120  \\\n",
       "0 -0.447204 -0.516943 -0.476097    ...     4.664526 -0.130893 -0.517853   \n",
       "1 -0.447204 -0.516943 -0.476097    ...    -0.537519  0.376582  1.650301   \n",
       "2 -0.447204 -0.516943 -0.476097    ...    -0.537519 -0.504821  3.314742   \n",
       "3 -0.447204 -0.516943 -0.476097    ...    -0.537519 -0.504821  3.358543   \n",
       "4 -0.447204 -0.516943 -0.476097    ...    -0.537519 -0.397985  3.336642   \n",
       "\n",
       "        121       122       123       124       125       126       127  \n",
       "0 -0.479069 -0.491777 -0.475926 -0.501479  0.124039  5.052102 -0.244152  \n",
       "1 -0.360755 -0.491777 -0.475926 -0.501479 -0.475640 -0.463781  0.676013  \n",
       "2  1.236482 -0.491777 -0.475926 -0.501479 -0.475640 -0.507385 -0.474193  \n",
       "3  1.354796 -0.491777 -0.475926 -0.501479 -0.475640 -0.507385 -0.474193  \n",
       "4 -0.272019 -0.491777 -0.475926 -0.501479 -0.475640 -0.507385 -0.359172  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling the sifts descriptors values for the clustering\n",
    "sifts_train_values = sifts_train.loc[:,'0':'127']\n",
    "sifts_test_values = sifts_test.loc[:,'0':'127']\n",
    "std_sc = StandardScaler()\n",
    "sifts_train_values = pd.DataFrame(std_sc.fit_transform(sifts_train_values))\n",
    "sifts_test_values = pd.DataFrame(std_sc.transform(sifts_test_values))\n",
    "sifts_train_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we perform a clustering of the sifts descriptors to regroup all the sifts that look alike. \n",
    "We use the K means method, with different values of k (100, 250, 750, 1000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Adding a column to the \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nb_clusters = [100, 250, 500, 750, 1000]\n",
    "for k in nb_clusters:\n",
    "    print(k)\n",
    "    name = 'cluster_km_' + str(k) \n",
    "    km = MiniBatchKMeans(n_init = 3, max_iter= 100, n_clusters = k)\n",
    "    km.fit(sifts_train_values)\n",
    "    sifts_train_values[name] = km.labels_\n",
    "    sifts_test_values[name] = km.predict(sifts_test_values)\n",
    "sifts_train_values['Id'] = sifts_train['Id']\n",
    "sifts_test_values['Id'] = sifts_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>cluster_km_100</th>\n",
       "      <th>cluster_km_250</th>\n",
       "      <th>cluster_km_500</th>\n",
       "      <th>cluster_km_750</th>\n",
       "      <th>cluster_km_1000</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>0.124039</td>\n",
       "      <td>5.052102</td>\n",
       "      <td>-0.244152</td>\n",
       "      <td>41</td>\n",
       "      <td>80</td>\n",
       "      <td>189</td>\n",
       "      <td>234</td>\n",
       "      <td>52</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.463781</td>\n",
       "      <td>0.676013</td>\n",
       "      <td>4</td>\n",
       "      <td>175</td>\n",
       "      <td>145</td>\n",
       "      <td>148</td>\n",
       "      <td>459</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.507385</td>\n",
       "      <td>-0.474193</td>\n",
       "      <td>10</td>\n",
       "      <td>228</td>\n",
       "      <td>423</td>\n",
       "      <td>208</td>\n",
       "      <td>416</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.507385</td>\n",
       "      <td>-0.474193</td>\n",
       "      <td>10</td>\n",
       "      <td>228</td>\n",
       "      <td>423</td>\n",
       "      <td>208</td>\n",
       "      <td>416</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.4831</td>\n",
       "      <td>-0.452598</td>\n",
       "      <td>-0.460596</td>\n",
       "      <td>-0.445141</td>\n",
       "      <td>-0.466192</td>\n",
       "      <td>-0.450662</td>\n",
       "      <td>-0.476716</td>\n",
       "      <td>-0.447204</td>\n",
       "      <td>-0.516943</td>\n",
       "      <td>-0.476097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501479</td>\n",
       "      <td>-0.475640</td>\n",
       "      <td>-0.507385</td>\n",
       "      <td>-0.359172</td>\n",
       "      <td>10</td>\n",
       "      <td>228</td>\n",
       "      <td>423</td>\n",
       "      <td>346</td>\n",
       "      <td>58</td>\n",
       "      <td>-1191173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6  \\\n",
       "0 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "1 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "2 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "3 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "4 -0.4831 -0.452598 -0.460596 -0.445141 -0.466192 -0.450662 -0.476716   \n",
       "\n",
       "          7         8         9   ...          124       125       126  \\\n",
       "0 -0.447204 -0.516943 -0.476097   ...    -0.501479  0.124039  5.052102   \n",
       "1 -0.447204 -0.516943 -0.476097   ...    -0.501479 -0.475640 -0.463781   \n",
       "2 -0.447204 -0.516943 -0.476097   ...    -0.501479 -0.475640 -0.507385   \n",
       "3 -0.447204 -0.516943 -0.476097   ...    -0.501479 -0.475640 -0.507385   \n",
       "4 -0.447204 -0.516943 -0.476097   ...    -0.501479 -0.475640 -0.507385   \n",
       "\n",
       "        127  cluster_km_100  cluster_km_250  cluster_km_500  cluster_km_750  \\\n",
       "0 -0.244152              41              80             189             234   \n",
       "1  0.676013               4             175             145             148   \n",
       "2 -0.474193              10             228             423             208   \n",
       "3 -0.474193              10             228             423             208   \n",
       "4 -0.359172              10             228             423             346   \n",
       "\n",
       "   cluster_km_1000       Id  \n",
       "0               52 -1191173  \n",
       "1              459 -1191173  \n",
       "2              416 -1191173  \n",
       "3              416 -1191173  \n",
       "4               58 -1191173  \n",
       "\n",
       "[5 rows x 134 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sifts_train_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sift_train_values, the columns cluster_km_k correspond to the label of cluster to which the sift is belonging. In the sift_test_values these columns correspond to the predicted cluster label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighting + Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cluster label for each sift, we will count the number of sift clusters by image, using 3 types of weighting:\n",
    "- Binary: 1 if the category is present in the picture, 0 otherwise.\n",
    "- Count: Number of time the category appears in the picture\n",
    "- Tf-Idf\n",
    "\n",
    "Once we obtain the final dataset, (1 line per picture, 1 column per cluster, value = weight), we will be able to train a model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_train = pd.read_csv('../Data/id_train.csv')\n",
    "binary_vectorizer = CountVectorizer(min_df=1, binary = True)\n",
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing different models with different number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each number k of clusters, we will try: \n",
    "- Logistic regression with different penalization (L1 and L2, different values of c)\n",
    "- Bernouilli Naive Bayes with different penalization\n",
    "- KNN with different number of neighbors and two types of weightings (uniform, distance)\n",
    "- SVM with different kernels\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "\n",
    "Each model is build on a sample of 0.8 of the sifts_train_binary dataset. Then we test its performance and the validation dataset (0.2 remaining pictures). We compute the maximal accuracy score and the proportion per class of well classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_j = -1 #nb of jobs for the grid search \n",
    "n_cv = 5#nb of cv dfolds for the grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary weigthing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "\t Logistic Regression\n",
      "('\\t Best params:', {'penalty': 'l2', 'multi_class': 'multinomial', 'C': 0.01, 'class_weight': None}, ':', 0.44828125000000002)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(250, 'clusters:')\n",
      "\t Logistic Regression\n",
      "('\\t Best params:', {'penalty': 'l2', 'multi_class': 'ovr', 'C': 0.01, 'class_weight': None}, ':', 0.50718750000000001)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "\t Logistic Regression\n",
      "('\\t Best params:', {'penalty': 'l2', 'multi_class': 'ovr', 'C': 0.01, 'class_weight': None}, ':', 0.54921874999999998)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "\t Logistic Regression\n",
      "('\\t Best params:', {'penalty': 'l2', 'multi_class': 'ovr', 'C': 0.001, 'class_weight': 'balanced'}, ':', 0.55843750000000003)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "\t Logistic Regression\n",
      "('\\t Best params:', {'penalty': 'l2', 'multi_class': 'ovr', 'C': 0.001, 'class_weight': 'balanced'}, ':', 0.56015625000000002)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "   \n",
    "    #Logistic Regression\n",
    "    print(\"\\t Logistic Regression\")\n",
    "    logreg = LogisticRegression(solver ='lbfgs')\n",
    "    parameters = {'C' : [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10], 'penalty': ['l2'], 'class_weight': ['balanced', None], 'multi_class' : ['ovr', 'multinomial']}\n",
    "    gs_logreg = GridSearchCV(logreg , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_logreg.predict(X_test))\n",
    "    print('\\t Best params:', gs_logreg.best_params_, ':', gs_logreg.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernouilli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "()\n",
      "\t Bernoulli NB\n",
      "('\\t Best params:', {'alpha': 50}, ':', 0.44515624999999998)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(250, 'clusters:')\n",
      "()\n",
      "\t Bernoulli NB\n",
      "('\\t Best params:', {'alpha': 75}, ':', 0.50390625)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "()\n",
      "\t Bernoulli NB\n",
      "('\\t Best params:', {'alpha': 75}, ':', 0.54203124999999996)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "()\n",
      "\t Bernoulli NB\n",
      "('\\t Best params:', {'alpha': 50}, ':', 0.56046874999999996)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "()\n",
      "\t Bernoulli NB\n",
      "('\\t Best params:', {'alpha': 25}, ':', 0.56562500000000004)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Bernouilli Naive Bayes\n",
    "    print()\n",
    "    print('\\t Bernoulli NB')\n",
    "    bnb = BernoulliNB()\n",
    "    parameters = {'alpha' : [0, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 100]}\n",
    "    gs_bnb = GridSearchCV(bnb , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_bnb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_bnb.predict(X_test))\n",
    "    print('\\t Best params:', gs_bnb.best_params_, ':', gs_bnb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "()\n",
      "\t Knn\n",
      "('\\t Best params:', {'n_neighbors': 75, 'weights': 'uniform'}, ':', 0.43281249999999999)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(250, 'clusters:')\n",
      "()\n",
      "\t Knn\n",
      "('\\t Best params:', {'n_neighbors': 25, 'weights': 'uniform'}, ':', 0.45687499999999998)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "()\n",
      "\t Knn\n",
      "('\\t Best params:', {'n_neighbors': 75, 'weights': 'distance'}, ':', 0.51406249999999998)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "()\n",
      "\t Knn\n",
      "('\\t Best params:', {'n_neighbors': 100, 'weights': 'uniform'}, ':', 0.52390625000000002)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "()\n",
      "\t Knn\n",
      "('\\t Best params:', {'n_neighbors': 150, 'weights': 'uniform'}, ':', 0.51359374999999996)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #KNN\n",
    "    print()\n",
    "    print('\\t Knn')\n",
    "    knn = KNeighborsClassifier(p = 1)\n",
    "    parameters = {'n_neighbors' : [5, 10, 25, 50, 75, 100, 150, 200, 250], 'weights' : ['uniform', 'distance']}\n",
    "    gs_knn = GridSearchCV(knn , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_knn.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_knn.predict(X_test))\n",
    "    print('\\t Best params:', gs_knn.best_params_, ':', gs_knn.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "()\n",
      "\t SVM\n",
      "('\\t Best params:', {'kernel': 'sigmoid', 'C': 0.75}, ':', 0.43125000000000002)\n",
      "('\\t', array([0, 0, 0, 1]))\n",
      "(250, 'clusters:')\n",
      "()\n",
      "\t SVM\n",
      "('\\t Best params:', {'kernel': 'rbf', 'C': 0.1}, ':', 0.48375000000000001)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "()\n",
      "\t SVM\n",
      "('\\t Best params:', {'kernel': 'poly', 'C': 10}, ':', 0.54468749999999999)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "()\n",
      "\t SVM\n",
      "('\\t Best params:', {'kernel': 'poly', 'C': 10}, ':', 0.55874999999999997)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "()\n",
      "\t SVM\n",
      "('\\t Best params:', {'kernel': 'rbf', 'C': 0.25}, ':', 0.55718749999999995)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #SVM\n",
    "    print()\n",
    "    print('\\t SVM')\n",
    "    svm = SVC(decision_function_shape='ovr', class_weight = 'balanced')\n",
    "    parameters = {'kernel':['linear', 'poly', 'rbf', 'sigmoid'], 'C':[0.1, 0.75, 1, 10]}\n",
    "    gs_svm = GridSearchCV(svm, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_svm.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_svm.predict(X_test))\n",
    "    print('\\t Best params:', gs_svm.best_params_, ':', gs_svm.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "()\n",
      "\t Random Forest\n",
      "('\\t Best params:', {'max_features': 0.2, 'n_estimators': 500, 'criterion': 'gini', 'max_depth': 15}, ':', 0.4478125)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(250, 'clusters:')\n",
      "()\n",
      "\t Random Forest\n",
      "('\\t Best params:', {'max_features': 0.2, 'n_estimators': 250, 'criterion': 'gini', 'max_depth': 15}, ':', 0.46921875000000002)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "()\n",
      "\t Random Forest\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 100, 'criterion': 'gini', 'max_depth': 10}, ':', 0.49421874999999998)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "()\n",
      "\t Random Forest\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 500, 'criterion': 'gini', 'max_depth': 15}, ':', 0.49296875000000001)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "()\n",
      "\t Random Forest\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 250, 'criterion': 'gini', 'max_depth': 15}, ':', 0.49796875000000002)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Random Forest\n",
    "    print()\n",
    "    print('\\t Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.2, 0.3, 'sqrt', 'log2'], 'max_depth': [6, 8, 10, 12, 15], 'criterion': ['gini', 'entropy']}\n",
    "    gs_rf = GridSearchCV(rf, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_rf.predict(X_test))\n",
    "    print('\\t Best params:', gs_rf.best_params_, ':', gs_rf.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 100, 'max_depth': 2}, ':', 0.453125)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(250, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 'log2', 'n_estimators': 250, 'max_depth': 3}, ':', 0.51484375000000004)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 'sqrt', 'n_estimators': 500, 'max_depth': 5}, ':', 0.55843750000000003)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 'log2', 'n_estimators': 500, 'max_depth': 2}, ':', 0.57218749999999996)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 'sqrt', 'n_estimators': 500, 'max_depth': 2}, ':', 0.57093749999999999)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    print()\n",
    "    print('\\t Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1,0.3, 'sqrt', 'log2', None], 'max_depth': [2, 5, 10]}\n",
    "    gs_gb = GridSearchCV(gb, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_gb.predict(X_test))\n",
    "    print('\\t Best params:', gs_gb.best_params_, ':', gs_gb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_binary = pd.DataFrame(binary_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_binary['Id'] = sifts_train_groups.index\n",
    "    sifts_train_binary = pd.merge(sifts_train_binary, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_binary.iloc[:,0:-2], sifts_train_binary['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "   \n",
    "    #Logistic Regression\n",
    "    print(\"\\t Logistic Regression\")\n",
    "    logreg = LogisticRegression()\n",
    "    parameters = {'C' : [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10], 'penalty': ['l1', 'l2'], 'class_weight': ['balanced', None], 'multi_class' : ['ovr', 'multinomial']}\n",
    "    gs_logreg = GridSearchCV(logreg , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_logreg.predict(X_test))\n",
    "    print('\\t Best params:', gs_logreg.best_params_, ':', gs_logreg.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "\n",
    "    #Bernouilli Naive Bayes\n",
    "    print()\n",
    "    print('\\t Bernoulli NB')\n",
    "    bnb = BernoulliNB()\n",
    "    parameters = {'alpha' : [0, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 100]}\n",
    "    gs_bnb = GridSearchCV(bnb , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_bnb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_bnb.predict(X_test))\n",
    "    print('\\t Best params:', gs_bnb.best_params_, ':', gs_bnb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "\n",
    "    #KNN\n",
    "    print()\n",
    "    print('\\t Knn')\n",
    "    knn = KNeighborsClassifier(p = 1)\n",
    "    parameters = {'n_neighbors' : [5, 10, 25, 50, 75, 100, 150, 200, 250], weights : ['uniform', 'distance']}\n",
    "    gs_knn = GridSearchCV(knn , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_knn.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_knn.predict(X_test))\n",
    "    print('\\t Best params:', gs_knn.best_params_, ':', gs_knn.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    #SVM\n",
    "    print()\n",
    "    print('\\t SVM')\n",
    "    svm = SVC(decision_function_shape='ovr', class_weight = 'balanced')\n",
    "    parameters = {'ker':['linear', 'poly', 'rbf', 'sigmoid'], 'C':[0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10]}\n",
    "    gs_svm = GridSearchCV(svm, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_svm.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_svm.predict(X_test))\n",
    "    print('\\t Best params:', gs_svm.best_params_, ':', gs_svm.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    #Random Forest\n",
    "    print()\n",
    "    print('\\t Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.2, 0.3, 'sqrt', 'log2'], 'max_depth': [6, 8, 10, 12, 15], 'criterion': ['gini', 'entropy']}\n",
    "    gs_rf = GridSearchCV(rf, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_rf.predict(X_test))\n",
    "    print('\\t Best params:', gs_rf.best_params_, ':', gs_rf.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    print()\n",
    "    print('\\t Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.3, 'sqrt', 'log2', None], 'max_depth': [2,3, 5, 10]}\n",
    "    gs_gb = GridSearchCV(gb, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_gb.predict(X_test))\n",
    "    print('\\t Best params:', gs_gb.best_params_, ':', gs_gb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Count Weigthing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "   \n",
    "    #Logistic Regression\n",
    "    print(\"\\t Logistic Regression\")\n",
    "    logreg = LogisticRegression(solver ='lbfgs')\n",
    "    parameters = {'C' : [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10], 'penalty': ['l2'], 'class_weight': ['balanced', None], 'multi_class' : ['ovr', 'multinomial']}\n",
    "    gs_logreg = GridSearchCV(logreg , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_logreg.predict(X_test))\n",
    "    print('\\t Best params:', gs_logreg.best_params_, ':', gs_logreg.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernouilli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Bernouilli Naive Bayes\n",
    "    print()\n",
    "    print('\\t Bernoulli NB')\n",
    "    bnb = BernoulliNB()\n",
    "    parameters = {'alpha' : [0, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 100]}\n",
    "    gs_bnb = GridSearchCV(bnb , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_bnb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_bnb.predict(X_test))\n",
    "    print('\\t Best params:', gs_bnb.best_params_, ':', gs_bnb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #KNN\n",
    "    print()\n",
    "    print('\\t Knn')\n",
    "    knn = KNeighborsClassifier(p = 1)\n",
    "    parameters = {'n_neighbors' : [5, 10, 25, 50, 75, 100, 150, 200, 250], 'weights' : ['uniform', 'distance']}\n",
    "    gs_knn = GridSearchCV(knn , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_knn.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_knn.predict(X_test))\n",
    "    print('\\t Best params:', gs_knn.best_params_, ':', gs_knn.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #SVM\n",
    "    print()\n",
    "    print('\\t SVM')\n",
    "    svm = SVC(decision_function_shape='ovr', class_weight = 'balanced')\n",
    "    parameters = {'kernel':[ 'poly', 'rbf', 'sigmoid'], 'C':[0.1, 0.75, 1, 10]}\n",
    "    gs_svm = GridSearchCV(svm, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_svm.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_svm.predict(X_test))\n",
    "    print('\\t Best params:', gs_svm.best_params_, ':', gs_svm.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Random Forest\n",
    "    print()\n",
    "    print('\\t Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.2, 0.3, 'sqrt', 'log2'], 'max_depth': [10, 15, 20, 50], 'criterion': ['gini', 'entropy']}\n",
    "    gs_rf = GridSearchCV(rf, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_rf.predict(X_test))\n",
    "    print('\\t Best params:', gs_rf.best_params_, ':', gs_rf.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    print()\n",
    "    print('\\t Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.3, 'sqrt', 'log2', None], 'max_depth': [2, 5, 10, 20]}\n",
    "    gs_gb = GridSearchCV(gb, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_gb.predict(X_test))\n",
    "    print('\\t Best params:', gs_gb.best_params_, ':', gs_gb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_count = pd.DataFrame(count_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_count['Id'] = sifts_train_groups.index\n",
    "    sifts_train_count = pd.merge(sifts_train_count, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_count.iloc[:,0:-2], sifts_train_count['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "        \n",
    "    #Logistic Regression\n",
    "    print(\"\\t Logistic Regression\")\n",
    "    logreg = LogisticRegression()\n",
    "    parameters = {'C' : [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10], 'penalty': ['l1', 'l2'], 'class_weight': ['balanced', None], 'multi_class' : ['ovr', 'multinomial']}\n",
    "    gs_logreg = GridSearchCV(logreg , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_logreg.predict(X_test))\n",
    "    print('\\t Best params:', gs_logreg.best_params_, ':', gs_logreg.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "\n",
    "    #Multinomial Naive Bayes\n",
    "    print()\n",
    "    print('\\t Multinomial NB')\n",
    "    bnb = MultinomialNB()\n",
    "    parameters = {'alpha' : [0, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 100]}\n",
    "    gs_bnb = GridSearchCV(bnb , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_bnb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_bnb.predict(X_test))\n",
    "    print('\\t Best params:', gs_bnb.best_params_, ':', gs_bnb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    #KNN\n",
    "    print()\n",
    "    print('\\t Knn')\n",
    "    knn = KNeighborsClassifier(p = 2)\n",
    "    parameters = {'n_neighbors' : [5, 10, 25, 50, 75, 100, 150, 200, 250], weights : ['uniform', 'distance']}\n",
    "    gs_knn = GridSearchCV(knn , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_knn.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_knn.predict(X_test))\n",
    "    print('\\t Best params:', gs_knn.best_params_, ':', gs_knn.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    print()\n",
    "    print('\\t SVM')\n",
    "    svm = SVC(decision_function_shape='ovr', class_weight = 'balanced')\n",
    "    parameters = {'ker':['linear', 'poly', 'rbf', 'sigmoid'], 'C':[0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10]}\n",
    "    gs_svm = GridSearchCV(svm, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_svm.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_svm.predict(X_test))\n",
    "    print('\\t Best params:', gs_svm.best_params_, ':', gs_svm.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    print()\n",
    "    print('\\t Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.2, 0.3, 'sqrt', 'log2'], 'max_depth': [6, 8, 10, 12, 15], 'criterion': ['gini', 'entropy']}\n",
    "    gs_rf = GridSearchCV(rf, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_rf.predict(X_test))\n",
    "    print('\\t Best params:', gs_rf.best_params_, ':', gs_rf.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    print()\n",
    "    print('\\t Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.3, 'sqrt', 'log2', None], 'max_depth': [2,3, 5, 10]}\n",
    "    gs_gb = GridSearchCV(gb, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_gb.predict(X_test))\n",
    "    print('\\t Best params:', gs_gb.best_params_, ':', gs_gb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "   \n",
    "    #Logistic Regression\n",
    "    print(\"\\t Logistic Regression\")\n",
    "    logreg = LogisticRegression(solver ='lbfgs')\n",
    "    parameters = {'C' : [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10], 'penalty': ['l2'], 'class_weight': ['balanced', None], 'multi_class' : ['ovr', 'multinomial']}\n",
    "    gs_logreg = GridSearchCV(logreg , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_logreg.predict(X_test))\n",
    "    print('\\t Best params:', gs_logreg.best_params_, ':', gs_logreg.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernouilli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Bernouilli Naive Bayes\n",
    "    print()\n",
    "    print('\\t Bernoulli NB')\n",
    "    bnb = BernoulliNB()\n",
    "    parameters = {'alpha' : [0, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 100]}\n",
    "    gs_bnb = GridSearchCV(bnb , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_bnb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_bnb.predict(X_test))\n",
    "    print('\\t Best params:', gs_bnb.best_params_, ':', gs_bnb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #KNN\n",
    "    print()\n",
    "    print('\\t Knn')\n",
    "    knn = KNeighborsClassifier(p = 1)\n",
    "    parameters = {'n_neighbors' : [5, 10, 25, 50, 75, 100, 150, 200, 250], 'weights' : ['uniform', 'distance']}\n",
    "    gs_knn = GridSearchCV(knn , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_knn.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_knn.predict(X_test))\n",
    "    print('\\t Best params:', gs_knn.best_params_, ':', gs_knn.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #SVM\n",
    "    print()\n",
    "    print('\\t SVM')\n",
    "    svm = SVC(decision_function_shape='ovr', class_weight = 'balanced')\n",
    "    parameters = {'ker':[ 'poly', 'rbf', 'sigmoid'], 'C':[0.1, 0.75, 10]}\n",
    "    gs_svm = GridSearchCV(svm, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_svm.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_svm.predict(X_test))\n",
    "    print('\\t Best params:', gs_svm.best_params_, ':', gs_svm.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Random Forest\n",
    "    print()\n",
    "    print('\\t Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.2, 0.3, 'sqrt', 'log2'], 'max_depth': [5, 10, 15, 20, 50], 'criterion': ['gini', 'entropy']}\n",
    "    gs_rf = GridSearchCV(rf, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_rf.predict(X_test))\n",
    "    print('\\t Best params:', gs_rf.best_params_, ':', gs_rf.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 100, 'max_depth': 5}, ':', 0.57140625)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(250, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 500, 'max_depth': 5}, ':', 0.58125000000000004)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(500, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 'sqrt', 'n_estimators': 500, 'max_depth': 5}, ':', 0.59375)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(750, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 0.3, 'n_estimators': 250, 'max_depth': 5}, ':', 0.59250000000000003)\n",
      "('\\t', array([0, 0, 0, 0]))\n",
      "(1000, 'clusters:')\n",
      "()\n",
      "\t Gradient Boosting\n",
      "('\\t Best params:', {'max_features': 'sqrt', 'n_estimators': 500, 'max_depth': 5}, ':', 0.59031250000000002)\n",
      "('\\t', array([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    print()\n",
    "    print('\\t Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.3, 'sqrt', 'log2', None], 'max_depth': [2, 5, 10, 20]}\n",
    "    gs_gb = GridSearchCV(gb, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_gb.predict(X_test))\n",
    "    print('\\t Best params:', gs_gb.best_params_, ':', gs_gb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for k in nb_clusters:\n",
    "    print(k, 'clusters:')\n",
    "    sifts_train_groups = sifts_train_values.groupby([\"Id\"])['cluster_km_'+str(k)].apply(list).apply(lambda x: ' '.join(map(str, x)))\n",
    "    sifts_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(sifts_train_groups).todense())\n",
    "    sifts_train_tfidf['Id'] = sifts_train_groups.index\n",
    "    sifts_train_tfidf = pd.merge(sifts_train_tfidf, id_train, how='inner', on='Id')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sifts_train_tfidf.iloc[:,0:-2], sifts_train_tfidf['label'], test_size=0.2, random_state=42)\n",
    "    y_test_count = np.array(y_test.value_counts().sort_index())\n",
    "    \n",
    "    #Logistic Regression\n",
    "    print(\"\\t Logistic Regression\")\n",
    "    logreg = LogisticRegression()\n",
    "    parameters = {'C' : [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10], 'penalty': ['l1', 'l2'], 'class_weight': ['balanced', None], 'multi_class' : ['ovr', 'multinomial']}\n",
    "    gs_logreg = GridSearchCV(logreg , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_logreg.predict(X_test))\n",
    "    print('\\t Best params:', gs_logreg.best_params_, ':', gs_logreg.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "\n",
    "    #Gaussian Naive Bayes\n",
    "    print()\n",
    "    print('\\t Gaussian NB')\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, gnb.predict(X_test))\n",
    "    conf_mat = confusion_matrix(y_test, gnb.predict(X_test))\n",
    "    print('\\t', acc)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    " \n",
    "    #KNN\n",
    "    print()\n",
    "    print('\\t Knn')\n",
    "    knn = KNeighborsClassifier(p = 2)\n",
    "    parameters = {'n_neighbors' : [5, 10, 25, 50, 75, 100, 150, 200, 250], weights : ['uniform', 'distance']}\n",
    "    gs_knn = GridSearchCV(knn , parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_knn.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_knn.predict(X_test))\n",
    "    print('\\t Best params:', gs_knn.best_params_, ':', gs_knn.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    #SVM\n",
    "    print()\n",
    "    print('\\t SVM')\n",
    "    svm = SVC(decision_function_shape='ovr', class_weight = 'balanced')\n",
    "    parameters = {'ker':['linear', 'poly', 'rbf', 'sigmoid'], 'C':[0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10]}\n",
    "    gs_svm = GridSearchCV(svm, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_svm.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_svm.predict(X_test))\n",
    "    print('\\t Best params:', gs_svm.best_params_, ':', gs_svm.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    #Random Forest\n",
    "    print()\n",
    "    print('\\t Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.2, 0.3, 'sqrt', 'log2'], 'max_depth': [6, 8, 10, 12, 15], 'criterion': ['gini', 'entropy']}\n",
    "    gs_rf = GridSearchCV(rf, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_rf.predict(X_test))\n",
    "    print('\\t Best params:', gs_rf.best_params_, ':', gs_rf.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)\n",
    "    \n",
    "    #Gradient Boosting\n",
    "    print()\n",
    "    print('\\t Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier()\n",
    "    parameters = {'n_estimators': [100, 250, 500], 'max_features':[0.1, 0.3, 'sqrt', 'log2', None], 'max_depth': [2,3, 5, 10]}\n",
    "    gs_gb = GridSearchCV(gb, parameters, scoring='accuracy', n_jobs=n_j, cv = n_cv)\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    conf_mat = confusion_matrix(y_test, gs_gb.predict(X_test))\n",
    "    print('\\t Best params:', gs_gb.best_params_, ':', gs_gb.best_score_)\n",
    "    print('\\t', np.diagonal(conf_mat)/y_test_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
